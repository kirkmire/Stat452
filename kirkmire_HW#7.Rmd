---
title: "kirkmire_HW#7"
author: "Colin Kirkmire"
date: "May 3, 2016"
output: html_document
---

(a)
```{r}
library(Stat2Data)
data(MedGPA)
med_data<-MedGPA[,c(1,2,3,5,10)]

library(ggplot2)

ggplot(med_data, aes(y=MCAT,x=GPA, shape=factor(Accept), color=factor(Accept)))+
  geom_point(size=3)+
  ggtitle("Score on MCAT vs GPA")+
  xlab("GPA (0 - 4)")+
  ylab("MCAT Score (18-48)")+
  labs(color="Acceptance")+
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14))
```

GPA appears to be a better predictor of acceptance since those with high GPAs past a certain threshold tend to be accepted irrespective of their MCAT score.  In contrast, many students with lesser GPAs that had roughly equivalent MCAT scores as those with higher GPAs were not accepted.

```{r}
med_table<-table(med_data$Sex,med_data$Accept)

chisq.test(med_table)
```
There is very little evidence against the null hypothesis that acceptance is independent of gender.


(b) Fit an additive logistic regression mode for predicting Acceptance from Sex, GPA and MCAT
```{r}
add_log_model<-glm(formula = Acceptance ~ Sex + GPA+ MCAT, family = binomial, data = med_data)

```

(c) Squared Terms
```{r}
sq_log_model<-glm(formula = Acceptance ~ Sex + GPA + MCAT + I(GPA^2)+ I(MCAT^2), family = binomial, data = med_data)

anova(add_log_model,sq_log_model,test="Chisq")
```

The deviance decreases only slightly whch provides little evidence against the null hypothesis that the added square terms are zero. Therefore, it is not neccesary or advisable to add the squared terms to the model.

(d) Pairwise Interactions
```{r}
pairwise_log_model<-glm(formula = Acceptance ~ Sex+GPA+MCAT+Sex:GPA+Sex:MCAT+GPA:MCAT, family = binomial, data = med_data)

anova(add_log_model,pairwise_log_model,test="Chisq")
```

The deviance decreases only slightly whch provides little evidence against the null hypothesis that the added square terms are zero. Therefore, it is not neccesary or advisable to add the squared terms to the model.

(e) Leverages and Cook's Distance
```{r}
#leverage#
lev<-hat(model.matrix(add_log_model))

plot(lev)

#Cooks Distance#
plot(add_log_model,which=c(4))


```
Case #54 has the highest leverage value becuase this student has both a very low MCAT score (18) and GPA (2.8). This case should be removed since this student is not representative of most students who apply to medical school.  In fact, this student recieved the lowest score possible on the MCAT which implies that they were not prepared to be taking the test in the first place and likely not a serious candidate for admission to medical school.

Case #40 has the highest value of Cook's distance.  This case has high leverage because the student had the fourth lowest GPA in the sample (3.14) and was still accepted to medical school. I do not believe this case warrants special treatment because there are several other students with similarly low GPAs that were also accepted. This situation does hint that there are additional variables that are important in determining admission apart from those in the model.

(f) Hosmer-Lemeshow Goodness-of-fit
```{r}
library(ResourceSelection)
hoslem.test(med_data$Acceptance, fitted(add_log_model), g=10)
```

The hosmer-lemeshow test shows that the observed numbers of accepted students compared to the predicted numbers of accepted students according to the logistical model are quite similar as indicated by the moderate chi-square statistic and high p-value.  There is very little evidence that our additaive logistical model is not a good fit for predicting acceptance.


(g) Compute Confidence Intervals for the parameters
```{r}

```


(h)Plot the probability of adission as a function of GPA for males and females seperately, assuming that MCAT score is fixed at its median.
```{r}
sum_add_log_model<-summary(add_log_model)

est_coef<-sum_add_log_model$coefficients[1:4,1]

cbind(confint(add_log_model,level=.95),est_coef)




```
Interpreting logit coefficients:

Sex: The log odds for a male being admitted are -1.5973 less than that of a female given that the other variables are held constant. This translates to odds of female acceptance being between 3.51 times as big as that of the male (95% CI: 16.50 times greater, 1.12 times less).

GPA: The log odds for admittance increase by an estimated 5.14 for every additional grade point. The 95% confidence interval for the multiplicitive increase in odds of admittance with every additional .1 grade point is between 6.7194 to 11203.5918 times bigger.




The estimated coefficients must be interpreted with care. Instead of the slope coefficients (B) being the rate of change in Y (the dependent variables) as X changes (as in the LP model or OLS regression), now the slope coefficient is interpreted as the rate of change in the "log odds" as X changes. This explanation is not very intuitive. It is possible to compute the more intuitive "marginal effect" of a continuous independent variable on the probability. The marginal effect is

dp/dB = f(BX)B
where f(.) is the density function of the cumulative probability distribution function [F(BX), which ranges from 0 to 1]. The marginal effects depend on the values of the independent variables, so, it is often useful to evaluate the marginal effects at the means of the independent variables. (SPSS doesn't have an option for the marginal effects. If you need to compute marginal effects you can use the LIMDEP statistical package which is available on the academic mainframe.)

An interpretation of the logit coefficient which is usually more intuitive (especially for dummy independent variables) is the "odds ratio"-- expB is the effect of the independent variable on the "odds ratio" [the odds ratio is the probability of the event divided by the probability of the nonevent]. For example, if expB3 =2, then a one unit change in X3 would make the event twice as likely (.67/.33) to occur. Odds ratios equal to 1 mean that there is a 50/50 chance that the event will occur with a small change in the independent variable. Negative coefficients lead to odds ratios less than one: if expB2 =.67, then a one unit change in X2 leads to the event being less likely (.40/.60) to occur. {Odds ratios less than 1 (negative coefficients) tend to be harder to interpret than odds ratios greater than one(positive coefficients).} Note that odds ratios for continuous independent variables tend to be close to one, this does NOT suggest that the coefficients are insignificant. Use the Wald statistic (see below) to test for statistical significance.








